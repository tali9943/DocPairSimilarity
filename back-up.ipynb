{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8f64ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_name_documents = './Databases/prova/prova4.jsonl'\n",
    "path_name_documents = './Databases/prova/prova50.jsonl'\n",
    "#path_name_documents = './Databases/prova/prova30000.jsonl'\n",
    "#path_name_documents = './Databases/prova/prova2000.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbaa2bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def time_it(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f\"Execution time: {end_time - start_time:.5f} seconds\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e28e1d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def stemmingLemming(filtered_tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Perform stemming or lemmatization on filtered tokens\n",
    "    \n",
    "    filtered_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    filtered_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "\n",
    "    return filtered_tokens\n",
    "    \n",
    " \n",
    "    \n",
    "\n",
    "def tokenize(path_name):\n",
    "    \n",
    "    with open(path_name, \"r\") as f:\n",
    "        data = f.readlines()\n",
    "\n",
    "        # Create an empty list to store the tokenized documents\n",
    "        tokenized_docs = []\n",
    "\n",
    "        # Loop through each line in the JSONL file\n",
    "        for line in data:\n",
    "            # Parse the JSON string into a Python dictionary\n",
    "            doc = json.loads(line)\n",
    "\n",
    "            # Extract the text from the dictionary\n",
    "            text = doc['text']\n",
    "            text = text.lower()  # Convert to lowercase\n",
    "            #text = re.sub(r'\\d+', '', text)  # Remove all numbers\n",
    "            text = text.translate(str.maketrans('', '', string.punctuation))  # Remove all punctuation\n",
    "\n",
    "            # Tokenize the text using NLTK\n",
    "            tokens = word_tokenize(text)\n",
    "            tokensStemLem = stemmingLemming(tokens)\n",
    "\n",
    "            # Add the tokenized document to the list\n",
    "            tokenized_docs.append(tokensStemLem)\n",
    "\n",
    "        # Print the tokenized documents\n",
    "    return tokenized_docs\n",
    "\n",
    "\n",
    "tokenized_docs = tokenize(path_name_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50ee5648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def calculateTFIDF(tokenized_docs):\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    # Fit and transform the tokenized documents into a TF-IDF matrix\n",
    "    tfidf_matrix = vectorizer.fit_transform([' '.join(doc) for doc in tokenized_docs])\n",
    "\n",
    "    # Get the feature names (tokens)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Return the TF-IDF matrix and the feature names\n",
    "    return tfidf_matrix, feature_names,vectorizer\n",
    "    \n",
    "        \n",
    "\n",
    "tfidf_matrix_docs, feature_names_docs,vectorizer  = calculateTFIDF(tokenized_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18f2d16",
   "metadata": {},
   "source": [
    "# DA QUI PARALLEL SPARK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90e548a",
   "metadata": {},
   "source": [
    "### UNA RIGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19012aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_row_values_tfidf(tfidf_matrix, doc_id):\n",
    "    num_docs = tfidf_matrix.shape[0]\n",
    "    # Estrai la riga specificata dalla matrice TF-IDF come un array densamente popolato\n",
    "    row_array = tfidf_matrix.toarray()[doc_id]\n",
    "    # Creazione della lista di coppie chiave-valore\n",
    "    row_values = [(index, value) for (index, value) in enumerate(row_array) if value != 0.0]\n",
    "    # Restituisci la lista di coppie chiave-valore\n",
    "    return doc_id, row_values,num_docs  \n",
    "\n",
    "\n",
    "doc_id, row_values, num_docs  = get_row_values_tfidf(tfidf_matrix_docs,0)\n",
    "#row_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a9b44a",
   "metadata": {},
   "source": [
    "### TUTTE LE RIGHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74ca85dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_row_values_tfidf(tfidf_matrix):\n",
    "    num_docs = tfidf_matrix.shape[0]\n",
    "    row_values_list = []\n",
    "    for doc_id in range(num_docs):\n",
    "        row_array = tfidf_matrix.toarray()[doc_id]\n",
    "        row_values = [(index, value) for (index, value) in enumerate(row_array) if value != 0.0]\n",
    "        row_values_list.append((doc_id, row_values))\n",
    "    return row_values_list\n",
    "\n",
    "\n",
    "all_rows_values_list = get_all_row_values_tfidf(tfidf_matrix_docs)\n",
    "#print(all_rows_values_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69fdb47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = 'C:/Users/lita4/anaconda3/python.exe'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'C:/Users/lita4/anaconda3/python.exe'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7252d169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sim(list1, list2):\n",
    "    similarity = abs(list1[0] - list2[0])  # Compute similarity as the absolute difference between the values\n",
    "    return similarity\n",
    "\n",
    "def b(document):\n",
    "    # Example function to calculate the threshold 'b' for a document\n",
    "    threshold = np.mean([value for _, value in document])\n",
    "    return threshold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ddb09b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione di mappatura\n",
    "def my_map(doc_id, document):\n",
    "    map_results = []\n",
    "    threshold = b(document)\n",
    "    for term, value in sorted(document):\n",
    "        if value > threshold:\n",
    "            map_results.append((term, (doc_id, document)))\n",
    "    return map_results\n",
    "\n",
    "#map_results = my_map(doc_id, document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a786ba2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def execution_map(num_docs,tfidf_matrix):\n",
    "    map_results\n",
    "    for i in range(0,num_docs):\n",
    "        doc_id, row_values, num_docs  = get_row_values_tfidf(tfidf_matrix_docs,i)\n",
    "        map_results.append(my_map(doc_id, row_values))\n",
    "    return map_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f659f0ec",
   "metadata": {},
   "source": [
    "### Funzione funzionante per python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "012fa518",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "\n",
    "def my_map_python(doc_id, document):\n",
    "    map_results = []\n",
    "    threshold = b(document)\n",
    "    for term, value in sorted(document):\n",
    "        if value > threshold:\n",
    "            map_results.append((term, (doc_id, document)))\n",
    "    return map_results\n",
    "\n",
    "\n",
    "def my_reduce_python(map_results, threshold):\n",
    "    emitted_results = []\n",
    "    for item in map_results:\n",
    "        t = item[0]\n",
    "        doc_id, document = item[1]\n",
    "        for index, (id1, d1) in enumerate(document):\n",
    "            for index2 in range(index + 1, len(document)):\n",
    "                id2, d2 = document[index2]\n",
    "                if t == max(id1, id2):\n",
    "                    similarity = sim([d1], [d2])\n",
    "                    if similarity >= threshold:\n",
    "                        emitted_results.append((id1, id2, similarity))\n",
    "    return emitted_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5287df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66a659b5",
   "metadata": {},
   "source": [
    "### Implementazioni per Spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4dd08aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark = SparkSession.builder.master(\"local[3]\").appName(\"all-doc-pairs-similarity.com\").config(\"spark.driver.memory\", \"10g\").getOrCreate()\n",
    "\n",
    "#sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99f440e",
   "metadata": {},
   "source": [
    "# -------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb0ce72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a43660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = get_all_row_values_tfidf(tfidf_matrix_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "958cb134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "def sim(list1, list2):\n",
    "    # Calcola la similarità come la somma delle differenze normalizzate tra i valori delle due liste\n",
    "    similarity = sum(abs(x - y) for (x, _), (y, _) in zip(list1, list2))\n",
    "    max_diff = max(x for x, _ in list1)\n",
    "    similarity = 1 - similarity / max_diff\n",
    "    return similarity\n",
    "\n",
    "\n",
    "# Funzione di mapping\n",
    "def my_map1(doc_id, document,threshold):\n",
    "    map_results = []\n",
    "    threshold = 0.2  # Soglia fissa\n",
    "    for term, value in sorted(document):\n",
    "        if value > threshold:\n",
    "            map_results.append((term, (doc_id, document)))\n",
    "    return map_results\n",
    "\n",
    "\n",
    "# Funzione di riduzione\n",
    "def my_reduce(t, doc_list, threshold):\n",
    "    emitted_results = []\n",
    "    for i, doc_item1 in enumerate(doc_list):\n",
    "        id1, d1 = doc_item1\n",
    "        for j in range(i + 1, len(doc_list)):\n",
    "            doc_item2 = doc_list[j]\n",
    "            id2, d2 = doc_item2\n",
    "            if id1 != id2:  # Confronta solo gli identificatori\n",
    "                similarity = sim(d1, d2)\n",
    "                if similarity >= threshold:\n",
    "                    emitted_results.append((id1, id2, similarity))\n",
    "    return emitted_results\n",
    "\n",
    "\n",
    "\n",
    "# Inizializza lo SparkContext\n",
    "sc = SparkContext()\n",
    "\n",
    "threshold = 0.3\n",
    "# Dati di input\n",
    "data = [(0, [(2, 0.41137791133379387), (4, 0.6445029922609534), (10, 0.6445029922609534)]),\n",
    "        (1, [(2, 0.41137791133379387), (3, 0.6445029922609534), (9, 0.6445029922609534)]),\n",
    "        (2, [(0, 0.5086718718935653), (5, 0.40104274646999605), (6, 0.40104274646999605), (7, 0.5086718718935653), (8, 0.40104274646999605)]),\n",
    "        (3, [(1, 0.5528163151092931), (2, 0.3528554929793508), (5, 0.43584673254990375), (6, 0.43584673254990375), (8, 0.43584673254990375)])]\n",
    "\n",
    "# Crea l'RDD di input\n",
    "input_rdd = sc.parallelize(datas)\n",
    "\n",
    "# Esegui la funzione di mapping\n",
    "mapped_rdd = input_rdd.flatMap(lambda x: my_map1(x[0], x[1],threshold))\n",
    "\n",
    "# Esegui la funzione di riduzione\n",
    "reduced_rdd = mapped_rdd.groupByKey().flatMap(lambda x: my_reduce(x[0], list(x[1]), 0.2))\n",
    "\n",
    "# Raccogli i risultati\n",
    "results = reduced_rdd.collect()\n",
    "\n",
    "# Elimina duplicati\n",
    "results = list(set(results))\n",
    "\n",
    "# Stampa i risultati\n",
    "for result in results:\n",
    "    print(result)\n",
    "\n",
    "# Ferma lo SparkContext\n",
    "sc.stop()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab3bfb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "769f32d1",
   "metadata": {},
   "source": [
    "# --------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7407ee1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 0.33333333333333337)\n",
      "(0, 1, 0.8)\n",
      "(2, 3, 0.25)\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "def sim(list1, list2):\n",
    "    # Calcola la similarità come la somma delle differenze normalizzate tra i valori delle due liste\n",
    "    similarity = sum(abs(x - y) for (x, _), (y, _) in zip(list1, list2))\n",
    "    max_diff = max(x for x, _ in list1)\n",
    "    similarity = 1 - similarity / max_diff\n",
    "    return similarity\n",
    "\n",
    "\n",
    "# Funzione di mapping\n",
    "def my_map1(doc_id, document,threshold):\n",
    "    map_results = []\n",
    "    threshold = 0.2  # Soglia fissa\n",
    "    for term, value in sorted(document):\n",
    "        if value > threshold:\n",
    "            map_results.append((term, (doc_id, document)))\n",
    "    return map_results\n",
    "\n",
    "\n",
    "# Funzione di riduzione\n",
    "def my_reduce(t, doc_list, threshold):\n",
    "    emitted_results = []\n",
    "    for i, doc_item1 in enumerate(doc_list):\n",
    "        id1, d1 = doc_item1\n",
    "        for j in range(i + 1, len(doc_list)):\n",
    "            doc_item2 = doc_list[j]\n",
    "            id2, d2 = doc_item2\n",
    "            similarity = sim(d1, d2)\n",
    "            if similarity >= threshold:\n",
    "                emitted_results.append((id1, id2, similarity))\n",
    "    return emitted_results\n",
    "\n",
    "\n",
    "# Inizializza lo SparkContext\n",
    "sc = SparkContext()\n",
    "\n",
    "threshold = 0.3\n",
    "# Dati di input\n",
    "data = [(0, [(2, 0.41137791133379387), (4, 0.6445029922609534), (10, 0.6445029922609534)]),\n",
    "        (1, [(2, 0.41137791133379387), (3, 0.6445029922609534), (9, 0.6445029922609534)]),\n",
    "        (2, [(0, 0.5086718718935653), (5, 0.40104274646999605), (6, 0.40104274646999605), (7, 0.5086718718935653), (8, 0.40104274646999605)]),\n",
    "        (3, [(1, 0.5528163151092931), (2, 0.3528554929793508), (5, 0.43584673254990375), (6, 0.43584673254990375), (8, 0.43584673254990375)])]\n",
    "\n",
    "# Crea l'RDD di input\n",
    "input_rdd = sc.parallelize(data)\n",
    "\n",
    "# Esegui la funzione di mapping\n",
    "mapped_rdd = input_rdd.flatMap(lambda x: my_map1(x[0], x[1],threshold))\n",
    "\n",
    "# Esegui la funzione di riduzione\n",
    "reduced_rdd = mapped_rdd.groupByKey().flatMap(lambda x: my_reduce(x[0], list(x[1]), 0.2))\n",
    "\n",
    "# Raccogli i risultati\n",
    "results = reduced_rdd.collect()\n",
    "\n",
    "# Elimina duplicati\n",
    "results = list(set(results))\n",
    "\n",
    "# Stampa i risultati\n",
    "for result in results:\n",
    "    print(result)\n",
    "\n",
    "# Ferma lo SparkContext\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138fa7fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4eceed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a9e2a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
