{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8f64ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_name_documents = './Databases/prova/prova.jsonl'\n",
    "#path_name_documents = './Databases/prova/prova4.jsonl'\n",
    "path_name_documents = './Databases/prova/prova10.jsonl'\n",
    "#path_name_documents = './Databases/prova/prova50.jsonl'\n",
    "#path_name_documents = './Databases/prova/prova2000.jsonl'\n",
    "#path_name_documents = './Databases/prova/prova30000.jsonl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbaa2bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def time_it(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f\"Execution time: {end_time - start_time:.5f} seconds\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e28e1d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def stemmingLemming(filtered_tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Perform stemming or lemmatization on filtered tokens\n",
    "    \n",
    "    filtered_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    filtered_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "\n",
    "    return filtered_tokens\n",
    "    \n",
    " \n",
    "    \n",
    "\n",
    "def tokenize(path_name):\n",
    "    \n",
    "    with open(path_name, \"r\") as f:\n",
    "        data = f.readlines()\n",
    "\n",
    "        # Create an empty list to store the tokenized documents\n",
    "        tokenized_docs = []\n",
    "\n",
    "        # Loop through each line in the JSONL file\n",
    "        for line in data:\n",
    "            # Parse the JSON string into a Python dictionary\n",
    "            doc = json.loads(line)\n",
    "\n",
    "            # Extract the text from the dictionary\n",
    "            text = doc['text']\n",
    "            text = text.lower()  # Convert to lowercase\n",
    "            #text = re.sub(r'\\d+', '', text)  # Remove all numbers\n",
    "            text = text.translate(str.maketrans('', '', string.punctuation))  # Remove all punctuation\n",
    "\n",
    "            # Tokenize the text using NLTK\n",
    "            tokens = word_tokenize(text)\n",
    "            tokensStemLem = stemmingLemming(tokens)\n",
    "\n",
    "            # Add the tokenized document to the list\n",
    "            tokenized_docs.append(tokensStemLem)\n",
    "\n",
    "        # Print the tokenized documents\n",
    "    return tokenized_docs\n",
    "\n",
    "\n",
    "tokenized_docs = tokenize(path_name_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50ee5648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def calculateTFIDF(tokenized_docs):\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    # Fit and transform the tokenized documents into a TF-IDF matrix\n",
    "    tfidf_matrix = vectorizer.fit_transform([' '.join(doc) for doc in tokenized_docs])\n",
    "\n",
    "    # Get the feature names (tokens)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Return the TF-IDF matrix and the feature names\n",
    "    return tfidf_matrix, feature_names,vectorizer\n",
    "    \n",
    "        \n",
    "\n",
    "tfidf_matrix_docs, feature_names_docs,vectorizer  = calculateTFIDF(tokenized_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18f2d16",
   "metadata": {},
   "source": [
    "# DA QUI PARALLEL SPARK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90e548a",
   "metadata": {},
   "source": [
    "### UNA RIGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19012aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_row_values_tfidf(tfidf_matrix, doc_id):\n",
    "    num_docs = tfidf_matrix.shape[0]\n",
    "    # Estrai la riga specificata dalla matrice TF-IDF come un array densamente popolato\n",
    "    row_array = tfidf_matrix.toarray()[doc_id]\n",
    "    # Creazione della lista di coppie chiave-valore\n",
    "    row_values = [(index, value) for (index, value) in enumerate(row_array) if value != 0.0]\n",
    "    # Restituisci la lista di coppie chiave-valore\n",
    "    return doc_id, row_values,num_docs  \n",
    "\n",
    "\n",
    "#doc_id, row_values, num_docs  = get_row_values_tfidf(tfidf_matrix_docs,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a9b44a",
   "metadata": {},
   "source": [
    "### TUTTE LE RIGHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74ca85dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_all_row_values_tfidf(tfidf_matrix):\n",
    "    num_docs = tfidf_matrix.shape[0]\n",
    "    row_values_list = []\n",
    "    for doc_id in range(num_docs):\n",
    "        row_array = tfidf_matrix.toarray()[doc_id]\n",
    "        row_values = [(index, value) for (index, value) in enumerate(row_array) if value != 0.0]\n",
    "        row_values_list.append((doc_id, row_values))\n",
    "    return row_values_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69fdb47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = 'C:/Users/lita4/anaconda3/python.exe'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'C:/Users/lita4/anaconda3/python.exe'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f659f0ec",
   "metadata": {},
   "source": [
    "### Funzione funzionante per python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a659b5",
   "metadata": {},
   "source": [
    "## Implementazioni per Spark "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab6889a",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01a2da11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def sim(list1, list2):\n",
    "    # Extract the values from the input lists\n",
    "    values1 = [x for x, _ in list1]\n",
    "    values2 = [x for x, _ in list2]\n",
    "    \n",
    "    # Pad the shorter list with zeros to match the dimensionality\n",
    "    max_length = max(len(values1), len(values2))\n",
    "    values1 += [0] * (max_length - len(values1))\n",
    "    values2 += [0] * (max_length - len(values2))\n",
    "    \n",
    "    # Calculate the cosine similarity between the two lists\n",
    "    similarity_matrix = cosine_similarity([values1], [values2])\n",
    "    similarity = similarity_matrix[0][0]\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "\n",
    "\n",
    "# Funzione di mapping\n",
    "def my_map(doc_id, document,threshold):\n",
    "    map_results = []\n",
    "    for term, value in sorted(document):\n",
    "        if value > threshold:\n",
    "            map_results.append((term, (doc_id, document)))\n",
    "    return map_results\n",
    "\n",
    "\n",
    "# Funzione di riduzione\n",
    "def my_reduce(t, doc_list, threshold):\n",
    "    emitted_results = []\n",
    "    for i, doc_item1 in enumerate(doc_list):\n",
    "        id1, d1 = doc_item1\n",
    "        for j in range(i + 1, len(doc_list)):\n",
    "            doc_item2 = doc_list[j]\n",
    "            id2, d2 = doc_item2\n",
    "            similarity = sim(d1, d2)\n",
    "            if similarity >= threshold:\n",
    "                emitted_results.append((id1+1, id2+1, similarity))\n",
    "    return emitted_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520cd9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_map1(doc_id, document, threshold):\n",
    "    map_results = []\n",
    "    \n",
    "    def b(document):\n",
    "        # Funzione ausiliaria per calcolare l'indice i più grande\n",
    "        # per cui la similarità è sotto la soglia di similarità desiderata\n",
    "        max_similarity = 0.0\n",
    "        max_index = 0\n",
    "        \n",
    "        for i in range(1, len(document) + 1):\n",
    "            subarray = document[:i]\n",
    "            similarity = calculate_similarity(subarray, vmax[:i])  # Sostituisci con il calcolo effettivo della similarità\n",
    "            \n",
    "            if similarity <= threshold:\n",
    "                max_similarity = similarity\n",
    "                max_index = i\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        return max_index\n",
    "    \n",
    "    sorted_document = sorted(document.items())\n",
    "    vmax = [value for _, value in sorted_document]\n",
    "    \n",
    "    for term, value in sorted_document:\n",
    "        if value > threshold:\n",
    "            map_results.append((term, [doc_id, document]))\n",
    "    \n",
    "    return map_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004e2d3d",
   "metadata": {},
   "source": [
    "### Execution with spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed273ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = get_all_row_values_tfidf(tfidf_matrix_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b8e3ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP\n",
      "REDUCE\n",
      "Time of execution 0.15300297737121582\n",
      "RESULTS\n",
      "(1, 43, 0.4036821730697277)\n",
      "(11, 31, 0.6456972907740861)\n",
      "(13, 20, 0.3672149017514845)\n",
      "(23, 39, 0.662805732685481)\n",
      "(27, 36, 0.8785909672237726)\n",
      "(30, 36, 0.9041935483968163)\n",
      "(32, 34, 0.661304032747174)\n",
      "(32, 49, 0.9206874495287574)\n",
      "(34, 49, 0.7254599281514812)\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "# Configura Spark per utilizzare 6 cores e 10 GB di RAM\n",
    "conf = SparkConf().setAppName(\"MyApp\").setMaster(\"local[8]\").set(\"spark.executor.memory\", \"12g\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "threshold = 0.3\n",
    "# Dati di input\n",
    "\n",
    "\n",
    "\n",
    "# Crea l'RDD di input\n",
    "input_rdd = sc.parallelize(datas)\n",
    "#print(input_rdd.collect())\n",
    "\n",
    "print(\"MAP\")\n",
    "# Esegui la funzione di mapping\n",
    "mapped_rdd = input_rdd.flatMap(lambda x: my_map(x[0], x[1], threshold))\n",
    "#print(mapped_rdd.collect())\n",
    "\n",
    "print(\"REDUCE\")\n",
    "# Esegui la funzione di riduzione\n",
    "reduced_rdd = mapped_rdd.groupByKey().flatMap(lambda x: my_reduce([0], list(x[1]), threshold))\n",
    "\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(\"Time of execution\",total_time)\n",
    "print(\"RESULTS\")\n",
    "# Raccogli i risultati\n",
    "results = reduced_rdd.collect()\n",
    "#print(results)\n",
    "# Elimina duplicati\n",
    "results = list(set(results))\n",
    "#print(results)\n",
    "for result in sorted(results):\n",
    "    print(result)\n",
    "\n",
    "# Ferma lo SparkContext\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ee21f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
