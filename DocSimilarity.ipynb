{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f97c1cc",
   "metadata": {},
   "source": [
    "# Documents Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3618ccc2",
   "metadata": {},
   "source": [
    "## Read Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4145dd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_name_documents = './Databases/prova/prova.jsonl'\n",
    "#path_name_documents = './Databases/prova/prova10.jsonl'\n",
    "#path_name_documents = './Databases/prova/prova50.jsonl'\n",
    "#path_name_documents = './Databases/prova/prova5000.jsonl' \n",
    "#path_name_documents = './Databases/prova/prova30000.jsonl'\n",
    "#path_name_documents = './Databases/prova/prova2000.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07b4bfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "def readFile(path_name):\n",
    "    # Load the JSONL file into a list\n",
    "    with open(path_name, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Convert each JSON object into a dictionary\n",
    "    dicts = [json.loads(line) for line in lines]\n",
    "\n",
    "    # Convert the dictionaries into arrays and stack them vertically\n",
    "    arrays = np.vstack([np.array(list(d.values())) for d in dicts])\n",
    "\n",
    "    # Convert the arrays into a list of lists\n",
    "    text = arrays.tolist()\n",
    "    \n",
    "    return text\n",
    "\n",
    "documents = readFile(path_name_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4f5b47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def time_it(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f\"Execution time: {end_time - start_time:.5f} seconds\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa45c87e",
   "metadata": {},
   "source": [
    "## Tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a147620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def stemmingLemming(filtered_tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Perform stemming or lemmatization on filtered tokens\n",
    "    \n",
    "    filtered_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    filtered_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "\n",
    "    return filtered_tokens\n",
    "    \n",
    " \n",
    "    \n",
    "\n",
    "def tokenize(path_name):\n",
    "    \n",
    "    with open(path_name, \"r\") as f:\n",
    "        data = f.readlines()\n",
    "\n",
    "        # Create an empty list to store the tokenized documents\n",
    "        tokenized_docs = []\n",
    "\n",
    "        # Loop through each line in the JSONL file\n",
    "        for line in data:\n",
    "            # Parse the JSON string into a Python dictionary\n",
    "            doc = json.loads(line)\n",
    "\n",
    "            # Extract the text from the dictionary\n",
    "            text = doc['text']\n",
    "            text = text.lower()  # Convert to lowercase\n",
    "            #text = re.sub(r'\\d+', '', text)  # Remove all numbers\n",
    "            text = text.translate(str.maketrans('', '', string.punctuation))  # Remove all punctuation\n",
    "\n",
    "            # Tokenize the text using NLTK\n",
    "            tokens = word_tokenize(text)\n",
    "            tokensStemLem = stemmingLemming(tokens)\n",
    "\n",
    "            # Add the tokenized document to the list\n",
    "            tokenized_docs.append(tokensStemLem)\n",
    "\n",
    "        # Print the tokenized documents\n",
    "    return tokenized_docs\n",
    "\n",
    "\n",
    "tokenized_docs = tokenize(path_name_documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d0ad69",
   "metadata": {},
   "source": [
    "# Sparse Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43553d58",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a261c20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def calculateTFIDF(tokenized_docs):\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    # Fit and transform the tokenized documents into a TF-IDF matrix\n",
    "    tfidf_matrix = vectorizer.fit_transform([' '.join(doc) for doc in tokenized_docs])\n",
    "\n",
    "    # Get the feature names (tokens)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Return the TF-IDF matrix and the feature names\n",
    "    return tfidf_matrix, feature_names,vectorizer\n",
    "    \n",
    "        \n",
    "\n",
    "tfidf_matrix_docs, feature_names_docs,vectorizer  = calculateTFIDF(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db987111",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce68baf",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b11ef999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc 1</th>\n",
       "      <th>Doc 2</th>\n",
       "      <th>Doc 3</th>\n",
       "      <th>Doc 4</th>\n",
       "      <th>Doc 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc 1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.483146</td>\n",
       "      <td>0.323318</td>\n",
       "      <td>0.323318</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc 2</th>\n",
       "      <td>0.483146</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc 3</th>\n",
       "      <td>0.323318</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc 4</th>\n",
       "      <td>0.323318</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc 5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Doc 1     Doc 2     Doc 3     Doc 4  Doc 5\n",
       "Doc 1  1.000000  0.483146  0.323318  0.323318    0.0\n",
       "Doc 2  0.483146  1.000000  0.000000  0.000000    0.0\n",
       "Doc 3  0.323318  0.000000  1.000000  1.000000    0.0\n",
       "Doc 4  0.323318  0.000000  1.000000  1.000000    0.0\n",
       "Doc 5  0.000000  0.000000  0.000000  0.000000    1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def similarity(tfidf_matrix):\n",
    "    # calcoliamo la cosine similarity tra i documenti\n",
    "    cos_sim = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "    # creiamo una tabella con le cosine similarity per ogni coppia di documenti\n",
    "    sim_table = pd.DataFrame(cos_sim, columns=['Doc ' + str(i+1) for i in range(cos_sim.shape[0])], index=['Doc ' + str(i+1) for i in range(cos_sim.shape[0])])\n",
    "    \n",
    "    return sim_table, cos_sim\n",
    "\n",
    "cos_sim_table, cos_sim = similarity(tfidf_matrix_docs)\n",
    "cos_sim_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5873dd86",
   "metadata": {},
   "source": [
    "## Sequential algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ecaefd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58f6f759",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "def extract_document_terms(tfidf_matrix):\n",
    "    matrix_coo = coo_matrix(tfidf_matrix)\n",
    "    data = matrix_coo.data\n",
    "    row = matrix_coo.row\n",
    "    col = matrix_coo.col\n",
    "\n",
    "    doc_terms = []\n",
    "    current_doc = -1\n",
    "    terms = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        doc_id = row[i]\n",
    "        term_id = col[i]\n",
    "        term_value = data[i]\n",
    "\n",
    "        if doc_id != current_doc:\n",
    "            if terms:\n",
    "                doc_terms.append((current_doc+1, terms))\n",
    "                terms = []\n",
    "            current_doc = doc_id\n",
    "\n",
    "        terms.append((term_id, term_value))\n",
    "\n",
    "    if terms:\n",
    "        doc_terms.append((current_doc, terms))\n",
    "\n",
    "    return doc_terms\n",
    "\n",
    "doc_info_list = extract_document_terms(tfidf_matrix_docs)\n",
    "#print(doc_info_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0ccd78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "650c182f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_map(doc_info):\n",
    "    mapped_doc = []\n",
    "\n",
    "    doc_id, terms = doc_info\n",
    "    \n",
    "    max_term_id = max(terms, key=lambda x: x[0])[0]\n",
    "    doc_terms = [(term_id, value) for term_id, value in terms]\n",
    "    mapped_doc.append((doc_id, max_term_id, doc_terms))\n",
    "\n",
    "    return mapped_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6ec7701",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def apply_my_map(doc_info_list):\n",
    "    total_mapped = []\n",
    "\n",
    "    for doc_info in doc_info_list:\n",
    "        mapped = my_map(doc_info)\n",
    "        total_mapped.extend(mapped)\n",
    "\n",
    "    return total_mapped\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb1254c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_reduce(docs, threshold):\n",
    "    pairs = []\n",
    "    docs_list = list(docs)  # Convert the docs iterator to a list\n",
    "\n",
    "    n_docs = len(docs_list)\n",
    "    print(n_docs)\n",
    "    \n",
    "    for i in range(n_docs - 1):\n",
    "        for j in range(i + 1, n_docs):\n",
    "            doc1_id, term_id, doc1 = docs_list[i]\n",
    "            doc2_id, _, doc2 = docs_list[j]\n",
    "            \n",
    "            \n",
    "            terms_1 = {t_id1: val1 for t_id1, val1 in doc1}\n",
    "            \n",
    "            terms_2 = {t_id2: val2 for t_id2, val2 in doc2}\n",
    "            print(\"doc1 \",doc1_id)\n",
    "            print(\"term1 \",terms_1)\n",
    "            print(\"doc2 \",doc2_id)\n",
    "            print(\"term2 \",terms_2)\n",
    "\n",
    "            common_terms = set(terms_1).intersection(terms_2)\n",
    "            \n",
    "            print(\"com\",common_terms)\n",
    "            \n",
    "            if not common_terms:\n",
    "                continue\n",
    "\n",
    "            max_term = max(common_terms)\n",
    "            print(\"max \", max_term)\n",
    "            print(\"term_id \", term_id)\n",
    "            print()\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "            sim = 0.0\n",
    "\n",
    "            for term in common_terms:\n",
    "                sim += terms_1[term] * terms_2[term]\n",
    "\n",
    "            if sim >= threshold:\n",
    "                if doc2_id == n_docs:\n",
    "                    doc2_id += 1\n",
    "                pair = ((doc1_id, doc2_id), sim)\n",
    "                pairs.append(pair)\n",
    "\n",
    "    return pairs\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c21ddd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_list = apply_my_map(doc_info_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2a9afbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "doc1  1\n",
      "term1  {6: 0.7694470729725092, 5: 0.6387105775654869}\n",
      "doc2  2\n",
      "term2  {2: 0.7782829228046183, 6: 0.6279137616509933}\n",
      "com {6}\n",
      "max  6\n",
      "term_id  6\n",
      "\n",
      "doc1  1\n",
      "term1  {6: 0.7694470729725092, 5: 0.6387105775654869}\n",
      "doc2  3\n",
      "term2  {1: 0.6098184563533858, 7: 0.6098184563533858, 5: 0.5062044059286201}\n",
      "com {5}\n",
      "max  5\n",
      "term_id  6\n",
      "\n",
      "doc1  1\n",
      "term1  {6: 0.7694470729725092, 5: 0.6387105775654869}\n",
      "doc2  4\n",
      "term2  {1: 0.6098184563533858, 7: 0.6098184563533858, 5: 0.5062044059286201}\n",
      "com {5}\n",
      "max  5\n",
      "term_id  6\n",
      "\n",
      "doc1  1\n",
      "term1  {6: 0.7694470729725092, 5: 0.6387105775654869}\n",
      "doc2  4\n",
      "term2  {4: 0.5773502691896258, 0: 0.5773502691896258, 3: 0.5773502691896258}\n",
      "com set()\n",
      "doc1  2\n",
      "term1  {2: 0.7782829228046183, 6: 0.6279137616509933}\n",
      "doc2  3\n",
      "term2  {1: 0.6098184563533858, 7: 0.6098184563533858, 5: 0.5062044059286201}\n",
      "com set()\n",
      "doc1  2\n",
      "term1  {2: 0.7782829228046183, 6: 0.6279137616509933}\n",
      "doc2  4\n",
      "term2  {1: 0.6098184563533858, 7: 0.6098184563533858, 5: 0.5062044059286201}\n",
      "com set()\n",
      "doc1  2\n",
      "term1  {2: 0.7782829228046183, 6: 0.6279137616509933}\n",
      "doc2  4\n",
      "term2  {4: 0.5773502691896258, 0: 0.5773502691896258, 3: 0.5773502691896258}\n",
      "com set()\n",
      "doc1  3\n",
      "term1  {1: 0.6098184563533858, 7: 0.6098184563533858, 5: 0.5062044059286201}\n",
      "doc2  4\n",
      "term2  {1: 0.6098184563533858, 7: 0.6098184563533858, 5: 0.5062044059286201}\n",
      "com {1, 5, 7}\n",
      "max  7\n",
      "term_id  7\n",
      "\n",
      "doc1  3\n",
      "term1  {1: 0.6098184563533858, 7: 0.6098184563533858, 5: 0.5062044059286201}\n",
      "doc2  4\n",
      "term2  {4: 0.5773502691896258, 0: 0.5773502691896258, 3: 0.5773502691896258}\n",
      "com set()\n",
      "doc1  4\n",
      "term1  {1: 0.6098184563533858, 7: 0.6098184563533858, 5: 0.5062044059286201}\n",
      "doc2  4\n",
      "term2  {4: 0.5773502691896258, 0: 0.5773502691896258, 3: 0.5773502691896258}\n",
      "com set()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[((1, 2), 0.48314640598151465),\n",
       " ((1, 3), 0.32331810847686315),\n",
       " ((1, 4), 0.32331810847686315),\n",
       " ((3, 4), 1.0)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = my_reduce(mapped_list,0.1)\n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80fe1f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c0e40a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fed70b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b23c216f",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa7be498",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = 'C:/Users/lita4/anaconda3/python.exe'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'C:/Users/lita4/anaconda3/python.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bb7d780",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_info_list = extract_document_terms(tfidf_matrix_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d1a55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf67cc1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
